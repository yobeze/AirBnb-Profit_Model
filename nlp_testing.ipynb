{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 100)\n",
    "# pd.set_option('max_colwidth', 500)\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "\n",
    "reviews = pd.read_csv('reviews.csv')\n",
    "# reviews.head()\n",
    "df_host = pd.read_csv('listings_cleaned.csv')\n",
    "# df_host.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing unnecessary columns\n",
    "#can do this much more efficiently later\n",
    "host_filtered = df_host.drop(df_host.iloc[:, 11:59], axis = 1)\n",
    "host_filtered.head()\n",
    "# reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_filtered.drop(['scrape_id', 'last_scraped', 'name', 'description', 'neighborhood_overview', 'picture_url', 'host_id', 'host_url', 'last_review'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_filtered.drop(host_filtered.iloc[:, 10:], axis = 1, inplace = True)\n",
    "host_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming columns for merge on ID/listng ID\n",
    "reviews.drop(['reviewer_id', 'id'], axis = 1, inplace = True)\n",
    "reviews.rename(columns = {'listing_id': 'id'}, inplace = True)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stars = host_filtered.drop(df_host.iloc[:, 1:2], axis = 1)\n",
    "df_stars.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(2, 3)\n",
    "fig.tight_layout()\n",
    "\n",
    "ax[0, 0].hist(df_stars[\"review_scores_value\"]) #row=0, col=0\n",
    "ax[0, 0].set_title('Value')\n",
    "ax[1, 0].hist(df_stars[\"review_scores_checkin\"]) #row=1, col=0\n",
    "ax[1, 0].set_title('Check In')\n",
    "ax[0, 1].hist(df_stars[\"review_scores_cleanliness\"]) #row=0, col=1\n",
    "ax[0, 1].set_title('Cleanliness')\n",
    "ax[1, 1].hist(df_stars[\"review_scores_communication\"]) #row=1, col=1\n",
    "ax[1, 1].set_title('Communication')\n",
    "ax[1, 2].hist(df_stars[\"review_scores_location\"]) #row=1, col=2\n",
    "ax[1 ,2].set_title('Location')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot score rating by loc\n",
    "#if host has multiple listings, take average\n",
    "\n",
    "#appending price col to host_filtered\n",
    "price = df_host['price']\n",
    "\n",
    "host_filtered = host_filtered.join(price)\n",
    "host_filtered.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(reviews, host_filtered, on = 'id')\n",
    "#Removing review scores for this df\n",
    "df_merged.drop(df_merged.iloc[:, 7:12], axis = 1, inplace = True)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check missing values\n",
    "df_merged.drop(['review_scores_value'], axis = 1, inplace = True)\n",
    "df_merged['comments'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import collections\n",
    "\n",
    "#convert text to lowercase\n",
    "def convert(lst): \n",
    "    return ([i for item in lst for i in item.lower().split()]) \n",
    "\n",
    "filtered_comments = convert(df_merged['comments'].fillna(\"\")) #Fill in missing reviews with blank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_word = collections.Counter(filtered_comments) #Count word frequency\n",
    "\n",
    "clean_word_count = pd.DataFrame(count_word.most_common(5),\n",
    "                             columns=['words', 'count'])\n",
    "\n",
    "clean_word_count.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(['english']))\n",
    "#View sample set of stopwords\n",
    "list(stop_words)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_review_cleaned = [word for word in filtered_comments if word not in stop_words]\n",
    "words_review_count = collections.Counter(words_review_cleaned)\n",
    "\n",
    "word_review_count_df = pd.DataFrame(words_review_count.most_common(15),\n",
    "                             columns=['words', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot horizontal bar graph\n",
    "word_review_count_df.sort_values(by='count').plot.barh(x='words',\n",
    "                      y='count',\n",
    "                      ax=ax,\n",
    "                      color=\"purple\")\n",
    "\n",
    "ax.set_title(\"Most Frequent Words\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.shape\n",
    "# df_sample_set = df_merged.iloc[0:100000, :]\n",
    "# df_sample_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying PScores with builtin Vader SIA\n",
    "SIA = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "positive sentiment: score >= 0.05\n",
    "neutral sentiment: score > -0.05 and score < 0.05\n",
    "negative sentiment: score <= -0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sample_set.drop(['listing_url', 'host_name', 'price'], axis = 1, inplace = True)\n",
    "df_merged.drop(['listing_url', 'host_name'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['comments'] = df_merged[\"comments\"].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sample_set.fillna(0, inplace = True)\n",
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More data clenaing\n",
    "Extracting root words and removing punctuation etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grabbing root words\n",
    "p_stemmer = PorterStemmer()\n",
    "def stem(sentence):\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [p_stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "# df_sample_set = df_sample_set[df_sample_set['comments'].notnull()]\n",
    "df_merged = df_merged[df_merged['comments'].notnull()]\n",
    "df_merged['comments'] = df_merged['comments'].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "stopwords_list = set(stopwords.words(\"english\"))\n",
    "#List of punctuation to remove\n",
    "#Handle html tags?\n",
    "punctuations = \"\"\"!()-![]{};:,+'\"\\,<>./?@#$%^&*_~Â\"\"\" \n",
    "\n",
    "def reviewParse(comments):\n",
    "    #Split the review into words\n",
    "    splitReview = comments.split()\n",
    "    #Takes punctuation out\n",
    "    parsedReview = \" \".join([word.translate(str.maketrans('', '', punctuations)) + \" \" for word in splitReview])\n",
    "    return parsedReview\n",
    "  \n",
    "def clean_review(comments):\n",
    "    #Makes all words lowercase\n",
    "    clean_words = []\n",
    "    splitReview = comments.split()\n",
    "    for w in splitReview:\n",
    "        if w.isalpha() and w not in stopwords_list:\n",
    "            clean_words.append(w.lower())\n",
    "    #Joins split words back into sentence\n",
    "    clean_review = \" \".join(clean_words)\n",
    "    return clean_review\n",
    "\n",
    "df_merged = df_merged[df_merged['comments'].notnull()]\n",
    "df_merged['comments'] = df_merged['comments'].apply(reviewParse).apply(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This does not properly rounds all values, no 1 or 2 ratings\n",
    "#Need to find way to properly round values down beforehand\n",
    "# df_sample_set['review_scores_rating'] = df_sample_set['review_scores_rating'].astype(np.int64)\n",
    "# df_sample_set.info()\n",
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_sample_set['Sentiment'] = df_sample_set['review_scores_rating'].round(decimals = 2).apply(sentiment)\n",
    "# # df_sample_set.loc('comments')\n",
    "# df_test = pd.DataFrame()\n",
    "# # df_test['sentiment'] = df_test['round_review'].apply(sentiment)\n",
    "# # print(type(df_clean['review_scores_rating']))\n",
    "# df_test.info()\n",
    "# # df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(df_merged['comments'])[:200000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tfidf(t, d, D) = tf(t, d) * idf(t, D)\n",
    "- t = term\n",
    "- d = document\n",
    "- D = set of documents\n",
    "- TF-IDF provides a weight\n",
    "- This weight is a statistical measure used to evaluate how important \n",
    "a word is to a document in a collection or corpus. \n",
    "- The importance increases proportionally to the number of times a \n",
    "word appears in the document but is offset by the frequency of \n",
    "the word in the corpus (data-set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running SIA on cleaned dataset\n",
    "SIA = SentimentIntensityAnalyzer()\n",
    "for sentence in df_merged['comments'].values[5:10]:\n",
    "    print(sentence)\n",
    "    ss = SIA.polarity_scores(sentence)\n",
    "    for k in sorted(ss):\n",
    "        print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation between original stars rating and price is low\n",
    "corr_1 = df_merged[\"review_scores_rating\"]\n",
    "corr_2 = df_merged[\"price\"]\n",
    "correlation = corr_2.corr(corr_1)\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    " \n",
    "#Settings for count vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, max_features = 2000) \n",
    " \n",
    "#Send all docs here \n",
    "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure([go.Bar(x = df_merged.review_scores_rating.value_counts().index, y = df_merged.review_scores_rating.value_counts().tolist())])\n",
    "fig.update_layout(\n",
    "    title=\"Values in each Sentiment\",\n",
    "    xaxis_title = \"Sentiment\",\n",
    "    yaxis_title = \"Values\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_set = df_merged.dropna()\n",
    "df_train_set.head(10)\n",
    "# df_train_set.to_csv(\"airbnb-train-set.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_set[\"sentiment_scores\"] = df_train_set[\"comments\"].apply(lambda x: SIA.polarity_scores(x))\n",
    "df_train_set = pd.concat([df_train_set.drop(['sentiment_scores'], axis = 1), df_train_set['sentiment_scores'].apply(pd.Series)], axis = 1)\n",
    "df_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_set['polarity'] = df_train_set['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer.get_feature_names()\n",
    "# df_train_set.drop_duplicates(inplace=True)\n",
    "X = tfidf_vectorizer_vectors.toarray()\n",
    "Y = df_train_set['review_scores_rating'].astype(int)[:200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV \n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, confusion_matrix, classification_report, roc_auc_score,roc_curve,auc\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Dividing into train and validation sets\n",
    "\n",
    "SEED = 123\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=SEED)\n",
    "dt.fit(X_train,y_train)\n",
    "y_pred_test = dt.predict(X_test)\n",
    "print(\"Training Accuracy score: \"+str(round(accuracy_score(y_train,dt.predict(X_train)),4)))\n",
    "print(\"Testing Accuracy score: \"+str(round(accuracy_score(y_test,dt.predict(X_test)),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Must assign neutral values to positive\n",
    "#Neutral scores tend to have positive sentiment utilizing neutral words\n",
    "print(classification_report(y_test, y_pred_test, target_names = ['ignore_1', 'ignore_2', 'positive', 'negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "#print('Confusion matrix\\n', cm)\n",
    "cm_matrix = pd.DataFrame(data=cm, columns=['Ignore', 'Ignore_2', 'Actual Negative', 'Actual Positive'], \n",
    "                        index=['Ignore', 'Ignore_2', 'Predict Negative', 'Predict Positive'])\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred_train = gnb.predict(X_train)\n",
    "y_pred_test = gnb.predict(X_test)\n",
    "print(\"Training Accuracy score: \" + str(round(accuracy_score(y_train, gnb.predict(X_train)), 4)))\n",
    "print(\"Testing Accuracy score: \" + str(round(accuracy_score(y_test, gnb.predict(X_test)), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_test, target_names=['ignore', 'ignore_2', 'positive', 'negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "#print('Confusion matrix\\n', cm)\n",
    "cm_matrix = pd.DataFrame(data=cm, columns=['ignore', 'ignore_2', 'Actual Negative', 'Actual Positive'], \n",
    "                        index=['ignore', 'ignore_2', 'Predict Negative', 'Predict Positive'])\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=SEED).fit(X_train, y_train)\n",
    "y_pred_train = lr.predict(X_train)\n",
    "y_pred_test = lr.predict(X_test)\n",
    "print(\"Training Accuracy score: \"+str(round(accuracy_score(y_train,lr.predict(X_train)),4)))\n",
    "print(\"Testing Accuracy score: \"+str(round(accuracy_score(y_test,lr.predict(X_test)),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_test, target_names=['ignore', 'ignore_2', 'positive', 'negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "#print('Confusion matrix\\n', cm)\n",
    "cm_matrix = pd.DataFrame(data=cm, columns=['ignore', 'ignore_2', 'Actual Positive', 'Actual Negative'], \n",
    "                        index=['ignore', 'ignore_2', 'Predict Positive', 'Predict Negative'])\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "print(\"Training Accuracy score: \"+str(round(accuracy_score(y_train,clf.predict(X_train)), 4)))\n",
    "print(\"Testing Accuracy score: \"+str(round(accuracy_score(y_test,clf.predict(X_test)), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_test, target_names=['ignore', 'ignore_2', 'positive', 'negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "#print('Confusion matrix\\n', cm)\n",
    "cm_matrix = pd.DataFrame(data=cm, columns=['ignore', 'ignore_2', 'Actual Positive', 'Actual Negative'], \n",
    "                        index=['ignore', 'ignore_2', 'Predict Positive', 'Predict Negative'])\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "classifiers = [('Decision Tree', dt),\n",
    "               ('Logistic Regression', lr),\n",
    "                ('Naive Bayes', gnb)\n",
    "              ]\n",
    "vc = VotingClassifier(estimators=classifiers)\n",
    "# Fit 'vc' to the traing set and predict test set labels\n",
    "vc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training Accuracy score: \" + str(round(accuracy_score(y_train,vc.predict(X_train)), 4)))\n",
    "print(\"Testing Accuracy score: \" + str(round(accuracy_score(y_test,vc.predict(X_test)), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizer setup for all models\n",
    "docs_2 = list(df_merged['comments'])[:214541]\n",
    "\n",
    "tfidf_vectorizer_2 = TfidfVectorizer(use_idf=True, max_features = 2000) \n",
    " \n",
    "#Send all docs here \n",
    "tfidf_vectorizer_vectors_2 = tfidf_vectorizer_2.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = tfidf_vectorizer_2.transform(docs_2)\n",
    "words_df = pd.DataFrame(vectors.toarray(), columns = tfidf_vectorizer_2.get_feature_names_out())\n",
    "words_df.head(-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df_clean = words_df.dropna()\n",
    "words_df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using all our models. \n",
    "\n",
    "# Logistic Regression predictions + probabilities\n",
    "df_train_set['pred_logreg'] = lr.predict(words_df)\n",
    "df_train_set['pred_logreg_proba'] = lr.predict_proba(words_df)[:,1]\n",
    "\n",
    "# Decision Tree predictions + probabilities\n",
    "df_train_set['pred_forest'] = dt.predict(words_df)\n",
    "df_train_set['pred_forest_proba'] = dt.predict_proba(words_df)[:,1]\n",
    "\n",
    "# Bayes predictions + probabilities\n",
    "df_train_set['pred_bayes'] = gnb.predict(words_df)\n",
    "df_train_set['pred_bayes_proba'] = gnb.predict_proba(words_df)[:,1]\n",
    "\n",
    "\n",
    "\n",
    "# df_train_set = pd.concat([df_train_set.drop(['sentiment_scores'], axis = 1), df_train_set['sentiment_scores'].apply(pd.Series)], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_set['rand_forest'] = clf.predict(words_df)\n",
    "df_train_set['rand_forest_prob'] = clf.predict_proba(words_df)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classifier predictions + probabilities\n",
    "df_train_set['vc_class'] = vc.predict(words_df)\n",
    "df_train_set['vc_class_prob'] = vc.predict_proba(words_df)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_set.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = gnb.predict(X_test)\n",
    "\n",
    "mean_squared_error(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "rmse = sqrt(mean_squared_error(y_test, y_preds))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_model_residuals(models,X,y):\n",
    "#     f, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, sharey=True)\n",
    "#     plt.title('Plotting residuals using training (blue) and test (green) data')\n",
    "#     mean_sq_e = []\n",
    "#     for m, ax in ((models[0], ax1),(models[1], ax2),(models[2], ax3)):\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "#         m[0].fit(X_train, y_train)\n",
    "#         y_preds_train = m[0].predict(X_train)\n",
    "#         y_preds_test = m[0].predict(X_test)\n",
    "#         ax.scatter(m[0].predict(X_train), y_preds_train - y_train,c='#2B94E9',s=40,alpha=0.5)\n",
    "#         ax.scatter(m[0].predict(X_test), y_preds_test - y_test,c='#94BA65',s=40)\n",
    "#         ax.hlines(y=0, xmin=0, xmax=100)\n",
    "#         ax.set_title(m[1])\n",
    "#         ax.set_ylabel('Residuals')\n",
    "#         mean_sq_e.append(\"Model {} with absolute error {}\".format(m[1], str(mean_absolute_error(y_test,y_preds_test))))\n",
    "#     plt.xlim([20,70])\n",
    "#     plt.ylim([-100,100])  \n",
    "#     plt.show()\n",
    "#     print(mean_sq_e)\n",
    "# models = np.array([(dt,'Decision Tree'), (lr,'Logistic Regression'), (clf,'Random Forest')])\n",
    "# compare_model_residuals(models, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.Series(lr.predict(X), name=\"sentiment\")\n",
    "results = pd.concat([predictions],axis=1)\n",
    "results.to_csv(\"airbnb-review-sentiment.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we successfully studied various models like Decision Tree, SVM, Naive Bayes and Logistic Regression and implemented them for the given dataset as part of the experiment along with a comparative analysis of various metrics and made the following observations.\n",
    "\n",
    "Naïve Bayes and Decision Tree are susceptible to noise if present in the dataset because when we reduced the number of features by considering only the most frequent words the accuracy and AUC score increased significantly.\n",
    "Logistic Regression and SVM performed almost same for the given dataset even with the initial number of features.\n",
    "We can increase Accuracy marginally by removing Named Entities using spacy and performing Lemmatization on top of that on all the models mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sentiment anaylsis columns\n",
    "# df_sample_set[\"sentiment_scores\"] = df_sample_set[\"comments\"].apply(lambda x: SIA.polarity_scores(x))\n",
    "# df_sample_set = pd.concat([df_sample_set.drop(['sentiment_scores'], axis = 1), df_sample_set['sentiment_scores'].apply(pd.Series)], axis = 1)\n",
    "# df_sample_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates pscores (builtin nltk sentiment analysis)\n",
    "pscores = [SIA.polarity_scores(comments) for comments in df_train_set['comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([score['compound'] for score in pscores]).plot(kind = 'hist')\n",
    "plt.title('Compound Scores')\n",
    "plt.xlabel('Scores')\n",
    "plt.ylabel('frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([score['neu'] for score in pscores]).plot(kind='hist')\n",
    "plt.title('Neutral')\n",
    "plt.xlabel('Scores')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([score['pos'] for score in pscores]).plot(kind='hist')\n",
    "plt.title('Positive Scores')\n",
    "plt.xlabel('Scores')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([score['neg'] for score in pscores]).plot(kind='hist', bins=25)\n",
    "plt.title('Negative Scores')\n",
    "plt.xlabel('Scores')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_reviews = pd.DataFrame()\n",
    "scored_reviews['review'] = [r for r in df_train_set['comments']]\n",
    "scored_reviews['compound'] = [score['compound'] for score in pscores]\n",
    "scored_reviews['negativity'] = [score['neg'] for score in pscores]\n",
    "scored_reviews['neutrality'] = [score['neu'] for score in pscores]\n",
    "scored_reviews['positivity'] = [score['pos'] for score in pscores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_reviews.query('negativity > 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not many negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_reviews.query('negativity > positivity').query('negativity > 0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: nltk built in sentiment analysis seems to be not as accurate as I'd like\n",
    "# marking comments negative even though overall sentiment is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords'])\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    Input: Text String (str)\n",
    "    \n",
    "    Process: \n",
    "    1. Tokenize text into tokens\n",
    "    2. Remove stop words\n",
    "    3. Lemmatize\n",
    "    \n",
    "    Output: List of text tokens for string\n",
    "    '''\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(w.lower().strip()) for w in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ML pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split, Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df_test['comments']\n",
    "Y = df_test['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit pipeline\n",
    "# Takes too long to run\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = pipeline.predict(X_test)\n",
    "\n",
    "mean_squared_error(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "rmse = sqrt(mean_squared_error(y_test, y_preds))\n",
    "rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
